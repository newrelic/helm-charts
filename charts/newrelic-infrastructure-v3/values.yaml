# nameOverride: ""
# fullnameOverride: ""

# -- Images used by the chart for the integration and agents.
# @default -- See `values.yaml`
images:
  pullSecrets: []
  # Image for the New Relic Infrastructure Agent sidecar.
  forwarder:
    pullPolicy: IfNotPresent
    # -- Tag for the agent sidecar.
    tag: 1.23.0
    # -- Image for the agent sidecar.
    repository: newrelic/k8s-events-forwarder
    registry: docker.io
  # Image for the New Relic Infrastructure Agent plus integrations.
  agent:
    pullPolicy: IfNotPresent
    # -- Tag for the agent and integrations bundle.
    tag: 2.8.2
    # -- Image for the agent and integrations bundle.
    repository: newrelic/infrastructure-bundle
    registry: docker.io
  # Image for the New Relic Kubernetes integration.
  integration:
    pullPolicy: IfNotPresent
    # -- Tag for the kubernetes integration.
    tag: 3.0.0
    # -- Image for the kubernetes integration.
    repository: newrelic/nri-kubernetes
    registry: docker.io

# -- Config that applies to all instances of the solution: kubelet, ksm, control plane and sidecars.
# @default -- See `values.yaml`
common:
  # Configuration entries that apply to all instances of the integration: kubelet, ksm and control plane.
  config:
    # common.config.interval -- (duration) Intervals larger than 40s are not supported and will cause the NR UI to not
    # behave properly. Any non-nil value will override the `lowDataMode` default.
    # @default -- `15s` if `lowDataMode == false`, `30s` otherwise.
    interval:
  # -- Config for the Infrastructure agent.
  # Will be used by the forwarder sidecars and the agent running integrations.
  # See: https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/
  agentConfig: {}

# lowDataMode -- (bool) Send less data by incrementing the interval from `15s` (the default when `lowDataMode` is `false` or `nil`) to `30s`.
# Non-nil values of `common.config.interval` will override this value.
# @default -- `false`
lowDataMode:

# kubelet -- Configuration for the DaemonSet that collects metrics from the Kubelet.
# @default -- See `values.yaml`
kubelet:
  # -- Enable kubelet monitoring.
  # Advanced users only. Setting this to `false` is not supported and will break the New Relic experience.
  enabled: true
  annotations: {}
  tolerations:
    - operator: "Exists"
      effect: "NoSchedule"
    - operator: "Exists"
      effect: "NoExecute"
  nodeSelector: {}
  affinity:
    nodeAffinity: {}
  extraEnv: []
  extraEnvFrom: []
  extraVolumes: []
  extraVolumeMounts: []
  resources:
    limits:
      memory: 300M
    requests:
      cpu: 100m
      memory: 150M
  config:
    # -- Timeout for the kubelet APIs contacted by the integration
    timeout: 10s
    # -- Number of retries after timeout expired
    retries: 3
  # port:
  # scheme:

# ksm -- Configuration for the Deployment that collects state metrics from KSM (kube-state-metrics).
# @default -- See `values.yaml`
ksm:
  # -- Enable cluster state monitoring.
  # Advanced users only. Setting this to `false` is not supported and will break the New Relic experience.
  enabled: true
  annotations: {}
  tolerations:
    - operator: "Exists"
      effect: "NoSchedule"
    - operator: "Exists"
      effect: "NoExecute"
  nodeSelector: {}
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - podAffinityTerm:
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app.kubernetes.io/name: kube-state-metrics
          weight: 100
    nodeAffinity: {}
  extraEnv: []
  extraEnvFrom: []
  extraVolumes: []
  extraVolumeMounts: []
  # -- Resources for the KSM scraper pod.
  # Keep in mind that sharding is not supported at the moment, so memory usage for this component ramps up quickly on
  # large clusters.
  # @default -- 100m/150M -/850M
  resources:
    limits:
      memory: 850M  # Bump me up if KSM pod shows restarts.
    requests:
      cpu: 100m
      memory: 150M
  config:
    # -- Timeout for the ksm API contacted by the integration
    timeout: 10s
    # -- Number of retries after timeout expired
    retries: 3
  # -- if specified autodiscovery is not performed and the specified URL is used
  # staticUrl: "http://test.io:8080/metrics"
  # -- the URL scheme is not discovered and the specified one is used
  # scheme: "http"
  # -- the port is not discovered and the specified one is used
  # port: 8080
  # -- only the pods matching this selector are taken into account during autodiscovery
  # selector: "app.kubernetes.io/name=kube-state-metrics"
  # -- only the pods matching this namespace taken into account during autodiscovery
  # namespace: "ksm-namespace"

# controlPlane -- Configuration for the control plane scraper.
# @default -- See `values.yaml`
controlPlane:
  # -- Deploy control plane monitoring component.
  enabled: true
  # -- Run Control Plane scraper with `hostNetwork` even if `privileged` is set to false.
  # `hostNetwork` is required for most control plane configurations, as they only accept connections from localhost.
  # This is meant to be used with DaemonSets over on-premise clusters.
  unprivilegedHostNetwork: false
  annotations: {}
  tolerations:
    - operator: "Exists"
      effect: "NoSchedule"
    - operator: "Exists"
      effect: "NoExecute"
  nodeSelector: {}
  # -- Affinity for the control plane DaemonSet.
  # @default -- Deployed only in master nodes.
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
          - matchExpressions:
              - key: node-role.kubernetes.io/controlplane
                operator: Exists
          - matchExpressions:
              - key: node-role.kubernetes.io/etcd
                operator: Exists
          - matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: Exists
  # -- How to deploy the control plane scraper. If autodiscovery is in use, it should be `DaemonSet`.
  # Advanced users using static endpoints set this to `Deployment` to avoid reporting metrics twice.
  kind: DaemonSet
  extraEnv: []
  extraEnvFrom: []
  extraVolumes: []
  extraVolumeMounts: []
  resources:
    limits:
      memory: 300M
    requests:
      cpu: 100m
      memory: 150M
  config:
    # -- Timeout for the Kubernetes APIs contacted by the integration
    timeout: 10s
    # -- Number of retries after timeout expired
    retries: 3
    # -- ETCD monitoring configuration
    # @default -- Common settings for most K8s distributions.
    etcd:
      # -- Enable etcd monitoring. Might require manual configuration in some environments.
      enabled: true
      # Discover etcd pods using the following namespaces and selectors.
      # If a pod matches the selectors, the scraper will attempt to reach it through the `endpoints` defined below.
      autodiscover:
        - selector: "tier=control-plane,component=etcd"
          namespace: kube-system
          # Set to true to consider only pods sharing the node with the scraper pod.
          # This should be set to `true` if Kind is Daemonset, `false` otherwise.
          matchNode: true
          # Try to reach etcd using the following endpoints.
          endpoints:
            - url: https://localhost:4001
              insecureSkipVerify: true
              auth:
                type: bearer
            - url: http://localhost:2381
        - selector: "k8s-app=etcd-manager-main"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:4001
              insecureSkipVerify: true
              auth:
                type: bearer
        - selector: "k8s-app=etcd"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:4001
              insecureSkipVerify: true
              auth:
                type: bearer
      # Openshift users might want to remove previous autodiscover entries and add this one instead.
      # Manual steps are required to create a secret containing the required TLS certificates to connect to etcd.
      # - selector: "k8s-app=etcd"
      #   namespace: kube-system
      #   matchNode: true
      #   endpoints:
      #     - url: https://localhost:9979
      #       insecureSkipVerify: true
      #       auth:
      #         type: mTLS
      #         mtls:
      #           secretName: secret-name
      #           secretNamespace: secret-namespace

      # -- staticEndpoint configuration.
      # It is possible to specify static endpoint to scrape. If specified 'autodiscover' section is ignored.
      # If set the static endpoint should be reachable, otherwise an error will be returned and the integration stops.
      # Notice that if deployed as a daemonSet and not as a Deployment setting static URLs could lead to duplicate data
      # staticEndpoint:
      #   url: https://url:port
      #   insecureSkipVerify: true
      #   auth: {}

    # -- Scheduler monitoring configuration
    # @default -- Common settings for most K8s distributions.
    scheduler:
      # -- Enable scheduler monitoring.
      enabled: true
      autodiscover:
        - selector: "tier=control-plane,component=kube-scheduler"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:10259
              insecureSkipVerify: true
              auth:
                type: bearer
        - selector: "k8s-app=kube-scheduler"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:10259
              insecureSkipVerify: true
              auth:
                type: bearer
        - selector: "app=openshift-kube-scheduler,scheduler=true"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:10259
              insecureSkipVerify: true
              auth:
                type: bearer
      # -- staticEndpoint configuration.
      # It is possible to specify static endpoint to scrape. If specified 'autodiscover' section is ignored.
      # If set the static endpoint should be reachable, otherwise an error will be returned and the integration stops.
      # Notice that if deployed as a daemonSet and not as a Deployment setting static URLs could lead to duplicate data
      # staticEndpoint:
      #   url: https://url:port
      #   insecureSkipVerify: true
      #   auth: {}

    # -- Controller manager monitoring configuration
    # @default -- Common settings for most K8s distributions.
    controllerManager:
      # -- Enable controller manager monitoring.
      enabled: true
      autodiscover:
        - selector: "tier=control-plane,component=kube-controller-manager"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:10257
              insecureSkipVerify: true
              auth:
                type: bearer
        - selector: "k8s-app=kube-controller-manager"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:10257
              insecureSkipVerify: true
              auth:
                type: bearer
        - selector: "app=kube-controller-manager,kube-controller-manager=true"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:10257
              insecureSkipVerify: true
              auth:
                type: bearer
             #  mtls:
             #    secretName: secret-name
             #    secretNamespace: secret-namespace
        - selector: "app=controller-manager,controller-manager=true"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:10257
              insecureSkipVerify: true
              auth:
                type: bearer
      # -- staticEndpoint configuration.
      # It is possible to specify static endpoint to scrape. If specified 'autodiscover' section is ignored.
      # If set the static endpoint should be reachable, otherwise an error will be returned and the integration stops.
      # Notice that if deployed as a daemonSet and not as a Deployment setting static URLs could lead to duplicate data
      # staticEndpoint:
      #   url: https://url:port
      #   insecureSkipVerify: true
      #   auth: {}

    # -- API Server monitoring configuration
    # @default -- Common settings for most K8s distributions.
    apiServer:
      # -- Enable API Server monitoring
      enabled: true
      autodiscover:
        - selector: "tier=control-plane,component=kube-apiserver"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:8443
              insecureSkipVerify: true
              auth:
                type: bearer
            # Endpoint distributions target: Kind(v1.22.1)
            - url: https://localhost:6443
              insecureSkipVerify: true
              auth:
                type: bearer
            - url: http://localhost:8080
        - selector: "k8s-app=kube-apiserver"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:8443
              insecureSkipVerify: true
              auth:
                type: bearer
            - url: http://localhost:8080
        - selector: "app=openshift-kube-apiserver,apiserver=true"
          namespace: kube-system
          matchNode: true
          endpoints:
            - url: https://localhost:8443
              insecureSkipVerify: true
              auth:
                type: bearer
      # -- staticEndpoint configuration.
      # It is possible to specify static endpoint to scrape. If specified 'autodiscover' section is ignored.
      # If set the static endpoint should be reachable, otherwise an error will be returned and the integration stops.
      # Notice that if deployed as a daemonSet and not as a Deployment setting static URLs could lead to duplicate data
      # staticEndpoint:
      #   url: https://url:port
      #   insecureSkipVerify: true
      #   auth: {}

# -- Update strategy for the DaemonSets deployed.
# @default -- See `values.yaml`
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1

# -- Custom attributes to be added to the data reported by all integrations reporting in the cluster.
customAttributes: {}

# -- Enable verbose logging for all components.
verboseLog: false

# -- Settings controlling ServiceAccount creation.
# @default -- See `values.yaml`
serviceAccount:
  # -- Whether the chart should automatically create the ServiceAccount objects required to run.
  create: true
  annotations: {}
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# -- Annotations to be added to all pods created by the integration.
podAnnotations: {}
# -- Labels to be added to all pods created by the integration.
podLabels: {}

# -- Pod scheduling priority
# Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
priorityClassName: ""

# -- Security context used in all the containers of the pods
# When `privileged == true`, the Kubelet scraper will run as root and ignore these settings.
# @default -- See `values.yaml`
securityContext:
  # OpenShift users might want to change the default UID:GID to be inside the allowed range.
  runAsUser: 1000
  runAsGroup: 2000
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true

# -- Run the integration with full access to the host filesystem and network.
# Running in this mode allows reporting fine-grained cpu, memory, process and network metrics for your nodes.
# Additionally, it allows control plane monitoring, which requires hostNetwork to work.
privileged: true

# -- Settings controlling RBAC objects creation.
rbac:
  # -- Whether the chart should automatically create the RBAC objects required to run.
  create: true
  # -- Whether the chart should create Pod Security Policy objects.
  pspEnabled: false

# -- Config files for other New Relic integrations that should run in this cluster.
integrations: {}
# If you wish to monitor services running on Kubernetes you can provide integrations
# configuration under `integrations`. You just need to create a new entry where
# the key is the filename of the configuration file and the value is the content of
# the integration configuration. The key must end in ".yaml" as this will be the
# filename generated and the Infrastructure agent only looks for YAML files.
# The data is the actual integration configuration as described in the spec here:
# https://docs.newrelic.com/docs/integrations/integrations-sdk/file-specifications/integration-configuration-file-specifications-agent-v180
# For example, if you wanted to monitor a Redis instance that has a label "app=sampleapp"
# you could do so by adding following entry:
#  nri-redis-sampleapp:
#    discovery:
#      command:
#        # Run NRI Discovery for Kubernetes
#        # https://github.com/newrelic/nri-discovery-kubernetes
#        exec: /var/db/newrelic-infra/nri-discovery-kubernetes
#        match:
#          label.app: sampleapp
#    integrations:
#      - name: nri-redis
#        env:
#          # using the discovered IP as the hostname address
#          HOSTNAME: ${discovery.ip}
#          PORT: 6379
#        labels:
#          env: test

# Collect detailed metrics from processes running in the host.
# @default -- [True only for accounts created before July 20, 2020.](https://docs.newrelic.com/docs/release-notes/infrastructure-release-notes/infrastructure-agent-release-notes/new-relic-infrastructure-agent-1120)
# enableProcessMetrics: false

# Send data to New Relic Staging. Requires an staging-specific License Key.
# nrStaging: false

# Prefix nodes display name with cluster to reduce chances of collisions
# prefixDisplayNameWithCluster: false

# 'true' will use the node name as the name for the "host",
#  note that it may cause data collision if the node name is the same in different clusters
#  and prefixDisplayNameWithCluster is not set to true.
# 'false' will use the host name as the name for the "host".
# useNodeNameAsDisplayName: true
