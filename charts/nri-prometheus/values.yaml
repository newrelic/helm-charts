# IMPORTANT: Specify your New Relic API key here.
# licenseKey:
#
# or Specify secret which contains New Relic API key
# customSecretName: secret_name
# customSecretLicenseKey: secret_key
#
# IMPORTANT: The Kubernetes cluster name
# https://docs.newrelic.com/docs/kubernetes-monitoring-integration
# cluster: ""
#
# The previous values can also be declared as global in order to be
# shared by other newrelic charts
#
# global:
#   licenseKey:
#   customSecretName: secret_name
#   customSecretLicenseKey: secret_key
#   cluster: ""

prometheusScrape: "true"

# If set, the `config.transformations` option present in this file will be ignored,
# and instead a default one excluding KSM and cAdvisor metrics will be used.
# Default: false
# lowDataMode: false

nameOverride: ""

fullnameOverride: ""

image:
  repository: newrelic/nri-prometheus
  tag: ""  # Defaults to chart's appVersion
  ## It is possible to specify docker registry credentials
  ## See https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  # pullSecrets:
    # - name: regsecret

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 200m
  #   memory: 512Mi
  # requests:
  #   cpu: 100m
  #   memory: 256Mi

rbac:
  # Specifies whether RBAC resources should be created
  create: true

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the name template
  name: ""
  # Specify any annotations to add to the ServiceAccount
  annotations:

# If you wish to provide your own config.yaml file include it under config:
# the sample config file is included here as an example.
config:
  # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data.
  # Default: true
  # standalone: true

  # Set the Metrics API url. It should not be changed unless required (for example to use FedRamp approved endpoints)
  # metric_api_url: <url>

  # How often the integration should run.
  # Default: "30s"
  # scrape_duration: "30s"

  # The HTTP client timeout when fetching data from targets.
  # Default: "5s"
  # scrape_timeout: "5s"

  # scrape_services Allows to enable scraping the service and not the endpoints behind.
  # When endpoints are scraped this is no longer needed
  scrape_services: true

  # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does.
  # Please notice that depending on the number of endpoints behind a service the load can increase considerably
  scrape_endpoints: false

  # How old must the entries used for calculating the counters delta be
  # before the telemetry emitter expires them.
  # Default: "5m"
  # telemetry_emitter_delta_expiration_age: "5m"

  # How often must the telemetry emitter check for expired delta entries.
  # Default: "5m"
  # telemetry_emitter_delta_expiration_check_interval: "5m"

  # Whether the integration should run in verbose mode or not.
  # Default: false
  verbose: false

  # Whether the integration should run in audit mode or not. Defaults to false.
  # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent.
  # It does not include verbose mode. This can lead to a high log volume, use with care.
  # Default: false
  audit: false

  # Whether the integration should skip TLS verification or not.
  # Default: false
  insecure_skip_verify: false

  # The label used to identify scrapeable targets.
  # Targets can be identified using a label or annotation.
  # Default: "prometheus.io/scrape"
  scrape_enabled_label: "prometheus.io/scrape"

  # Whether k8s nodes need to be labelled to be scraped or not.
  # Default: true
  require_scrape_enabled_label_for_nodes: true

  # Number of worker threads used for scraping targets.
  # For large clusters with many (>400) targets, slowly increase until scrape
  # time falls between the desired `scrape_duration`.
  # Increasing this value too much will result in huge memory consumption if too
  # many metrics are being scraped.
  # Default: 4
  # worker_threads: 4

  # Maximum number of metrics to keep in memory until a report is triggered.
  # Changing this value is not recommended unless instructed by the New Relic support team.
  # max_stored_metrics: 10000

  # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms.
  # Changing this value is not recommended unless instructed by the New Relic support team.
  # min_emitter_harvest_period: 200ms

  # targets:
  #   - description: Secure etcd example
  #     urls: ["https://192.168.3.1:2379", "https://192.168.3.2:2379", "https://192.168.3.3:2379"]
  #     tls_config:
  #       ca_file_path: "/etc/etcd/etcd-client-ca.crt"
  #       cert_file_path: "/etc/etcd/etcd-client.crt"
  #       key_file_path: "/etc/etcd/etcd-client.key"

  # Proxy to be used by the emitters when submitting metrics. It should be
  # in the format [scheme]://[domain]:[port].
  # The emitter is the component in charge of sending the scraped metrics.
  # This proxy won't be used when scraping metrics from the targets.
  # By default it's empty, meaning that no proxy will be used.
  # emitter_proxy: "http://localhost:8888"

  # Certificate to add to the root CA that the emitter will use when
  # verifying server certificates.
  # If left empty, TLS uses the host's root CA set.
  # emitter_ca_file: "/path/to/cert/server.pem"

  # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account
  # having limited privileges.
  # Default: false
  # disable_autodiscovery: false

  # Whether the emitter should skip TLS verification when submitting data.
  # Default: false
  # emitter_insecure_skip_verify: false

  # Histogram support is based on New Relic's guidelines for higher
  # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md.
  # To better support visualization of this data, percentiles are calculated
  # based on the histogram metrics and sent to New Relic.
  # By default, the following percentiles are calculated: 50, 95 and 99.
  #
  # percentiles:
  #   - 50
  #   - 95
  #   - 99
  #
  # integration definition files required to map metrics to entities
  # definition_files_path: /etc/newrelic-infra/definition-files

  # If lowDataMode is se to true the config.transformations option will be ignored
  transformations: []
  # - description: "Custom transformation Example"
  #   rename_attributes:
  #     - metric_prefix: ""
  #       attributes:
  #         container_name: "containerName"
  #         pod_name: "podName"
  #         namespace: "namespaceName"
  #         node: "nodeName"
  #         container: "containerName"
  #         pod: "podName"
  #         deployment: "deploymentName"
  #   ignore_metrics:
  #     # Ignore the following metrics.
  #     # These metrics are already collected by the New Relic Kubernetes Integration.
  #     - prefixes:
  #       - kube_daemonset_
  #       - kube_deployment_
  #       - kube_endpoint_
  #       - kube_namespace_
  #       - kube_node_
  #       - kube_persistentvolume_
  #       - kube_pod_
  #       - kube_replicaset_
  #       - kube_service_
  #       - kube_statefulset_
  #   copy_attributes:
  #     # Copy all the labels from the timeseries with metric name
  #     # `kube_hpa_labels` into every timeseries with a metric name that
  #     # starts with `kube_hpa_` only if they share the same `namespace`
  #     # and `hpa` labels.
  #     - from_metric: "kube_hpa_labels"
  #       to_metrics: "kube_hpa_"
  #       match_by:
  #         - namespace
  #         - hpa
  #     - from_metric: "kube_daemonset_labels"
  #       to_metrics: "kube_daemonset_"
  #       match_by:
  #         - namespace
  #         - daemonset
  #     - from_metric: "kube_statefulset_labels"
  #       to_metrics: "kube_statefulset_"
  #       match_by:
  #         - namespace
  #         - statefulset
  #     - from_metric: "kube_endpoint_labels"
  #       to_metrics: "kube_endpoint_"
  #       match_by:
  #         - namespace
  #         - endpoint
  #     - from_metric: "kube_service_labels"
  #       to_metrics: "kube_service_"
  #       match_by:
  #         - namespace
  #         - service
  #     - from_metric: "kube_node_labels"
  #       to_metrics: "kube_node_"
  #       match_by:
  #         - namespace
  #         - node

# If you wish to provide additional annotations to apply to the pod(s), specify
# them here
# podAnnotations:

# If you wish to provide a pod security context, specify it here
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
# Kind: PodSecurityContext
# podSecurityContext:

# Pod scheduling priority
# Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
# priorityClassName: high-priority

nodeSelector: {}

tolerations: []

affinity: {}

# Sends data to staging, can be set as a global.
# global.nrStaging
nrStaging: false
