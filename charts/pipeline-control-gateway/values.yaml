## Default values for pipeline-control-gateway
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

fullnameOverride: "pipeline-control-gateway"
autoscaling:
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 60

ports:
  nrHttp: 80
  otlpHttp: 4318
  otlpGrpc: 4317
  healthHttp: 13133
  prometheus: 8888


image:
  # -- OTel collector image to be deployed. You can use your own collector as long it accomplish the following requirements mentioned below.
  repository: saitharunsajjanapu568/foobar
  # -- The pull policy is defaulted to IfNotPresent, which skips pulling an image if it already exists. If pullPolicy is defined without a specific value, it is also set to Always.
  pullPolicy: IfNotPresent
  # --  Overrides the image tag whose default is the chart appVersion.
  tag: "0.5.2"

# -- Name of the Kubernetes cluster monitored. Mandatory. Can be configured also with `global.cluster`
cluster: ""
# -- This set this license key to use. Can be configured also with `global.licenseKey`
licenseKey: ""
# -- In case you don't want to have the license key in you values, this allows you to point to a user created secret to get the key from there. Can be configured also with `global.customSecretName`
customSecretName: ""
# -- In case you don't want to have the license key in you values, this allows you to point to which secret key is the license key located. Can be configured also with `global.customSecretLicenseKey`
customSecretLicenseKey: ""

# -- Additional labels for chart pods
podLabels: {}
# -- Additional labels for chart objects
labels: {}

# -- Sets pod's priorityClassName. Can be configured also with `global.priorityClassName`
priorityClassName: ""

# -- Sets pod's dnsConfig. Can be configured also with `global.dnsConfig`
dnsConfig: {}

# -- Run the integration with full access to the host filesystem and network.
# Running in this mode allows reporting fine-grained cpu, memory, process and network metrics for your nodes.
# @default -- `true`
privileged: true


deployment:
  # -- Sets deployment pod node selector. Overrides `nodeSelector` and `global.nodeSelector`
  nodeSelector: {}
  # -- Sets deployment pod tolerations. Overrides `tolerations` and `global.tolerations`
  tolerations: []
  # -- Sets deployment pod affinities. Overrides `affinity` and `global.affinity`
  affinity: {}
  # -- Annotations to be added to the deployment.
  podAnnotations: {}
  # -- Sets security context (at pod level) for the deployment. Overrides `podSecurityContext` and `global.podSecurityContext`
  podSecurityContext: {}
  # -- Sets security context (at container level) for the deployment. Overrides `containerSecurityContext` and `global.containerSecurityContext`
  containerSecurityContext: {}
  # -- Sets resources for the deployment.
  resources:
    requests:
      memory: "2048Mi"
      cpu: "1000m"
    limits:
      memory: "2048Mi"
      cpu: "1000m"
  # -- Settings for deployment configmap
  # @default -- See `values.yaml`
  configMap:
    # -- OpenTelemetry config for the deployment. If set, overrides default config and disables configuration parameters for the deployment.
    config: {}

# -- Sets all pods' node selector. Can be configured also with `global.nodeSelector`
nodeSelector: {}
# -- Sets all pods' tolerations to node taints. Can be configured also with `global.tolerations`
tolerations: []
# -- Sets all pods' affinities. Can be configured also with `global.affinity`
affinity: {}
# -- Sets all security contexts (at pod level). Can be configured also with `global.securityContext.pod`
podSecurityContext: {}
# -- Sets all security context (at container level). Can be configured also with `global.securityContext.container`
containerSecurityContext: {}

rbac:
  # -- Specifies whether RBAC resources should be created
  create: true

# -- Settings controlling ServiceAccount creation
# @default -- See `values.yaml`
serviceAccount:
  # serviceAccount.create -- (bool) Specifies whether a ServiceAccount should be created
  # @default -- `true`
  create:
  # If not set and create is true, a name is generated using the fullname template
  name: ""
  # Specify any annotations to add to the ServiceAccount
  annotations:

# -- (bool) Sets the debug logs to this integration or all integrations if it is set globally. Can be configured also with `global.verboseLog`
# @default -- `false`
verboseLog:

# -- (bool) Send the metrics to the staging backend. Requires a valid staging license key. Can be configured also with `global.nrStaging`
# @default -- `false`
nrStaging:

# -- (bool) Send only the [metrics required](https://github.com/newrelic/helm-charts/tree/master/charts/nr-k8s-otel-collector/docs/metrics-lowDataMode.md) to light up the NR kubernetes UI, this agent defaults to setting lowDataMode true, but if this setting is unset, lowDataMode will be set to false
# @default -- `false`
lowDataMode:

# -- Complete OpenTelemetry pipeline configuration as YAML string. When provided, this will override all other OpenTelemetry configuration sections.
# @default -- `""`
generated: |
  extensions:
    zpages:
    healthcheckv2:
      use_v2: true
      component_health:
        include_permanent_errors: false
        include_recoverable_errors: true
        recovery_duration: 5m
      http:
        endpoint: ${env:MY_POD_IP}:13133
        status:
          enabled: true
          path: "/health/status"
        config:
          enabled: true
          path: "/health/config"

  receivers:
    nrproprietaryreceiver:
      proxy: false
      enable_runtime_metrics: true
      server:
        endpoint: "0.0.0.0:80"
      nr_host: "staging-collector.newrelic.com"
      endpoints:
        infra_event_api_endpoint: "https://staging-infra-api.newrelic.com"
        log_api_endpoint: "https://staging-log-api.newrelic.com"
        event_api_endpoint: "https://staging-insights-collector.newrelic.com"
        traces_endpoint: "https://staging-trace-api.newrelic.com"
        metrics_endpoint: "https://staging-metric-api.newrelic.com"
      logfilter:
        enabled: false
      enable_default_host: false
      client:
        compression: "gzip"
    otlp:
      protocols:
        http:
          endpoint: ${env:MY_POD_IP}:4318
        grpc:
          endpoint: ${env:MY_POD_IP}:4317
    prometheus/monitoring:
      config:
        scrape_configs:
          - job_name: 'pipeline-gateway-monitoring'
            scrape_interval: 15s
            static_configs:
              - targets: [ '0.0.0.0:8888' ]
                labels:
                  version: "1.2.0"
                  podName: '${env:MY_POD_NAME}'
                  clusterName: '${env:CLUSTER_NAME}'
                  serviceName: 'pipeline-control-gateway'
            metric_relabel_configs:
              - action: labeldrop
                regex: 'service_version|service_name|service_instance_id'
              - source_labels: [ __name__ ]
                regex: 'godebug_.*'
                action: drop

  processors:
    memory_limiter:
      check_interval: 1s
      limit_mib: 100
    nrprocessor:
      queue:
        enabled: true
        queue_size: 100
      queries: []

  exporters:
    otlp:
      endpoint: https://otlp.nr-data.net:443
      headers:
        api-key: ${env:NEW_RELIC_LICENSE_KEY}
    otlphttp:
      endpoint: https://otlp.nr-data.net/v1/traces
      headers:
        api-key: ${env:NEW_RELIC_LICENSE_KEY}
    nrcollectorexporter:
      endpoint: https://log-api.newrelic.com/log/v1
      retry_on_failure:
        enabled: true
        initial_interval: 100ms
        max_interval: 500ms
        max_elapsed_time: 5s
      timeout: 10s
      sending_queue:
        enabled: false
      compression: gzip
      encoding: json
      nr_license_key: ${env:NEW_RELIC_LICENSE_KEY}

  service:
    extensions: [ healthcheckv2 ]

    pipelines:
      logs/nr:
        receivers: [nrproprietaryreceiver]
        processors: [nrprocessor]
        exporters: [nrcollectorexporter]
      logs/otlp:
        receivers: [ otlp ]
        processors: [ nrprocessor ]
        exporters: [ otlp ]
      metrics/nr:
        receivers: [nrproprietaryreceiver]
        processors: [nrprocessor]
        exporters: [nrcollectorexporter]
      metrics/otlp:
        receivers: [otlp]
        processors: [ nrprocessor]
        exporters: [otlp]
      traces/otlp:
        receivers: [otlp]
        processors: [nrprocessor]
        exporters: [ otlp ]
      traces/nr:
        receivers: [ nrproprietaryreceiver ]
        processors: [ nrprocessor ]
        exporters: [ nrcollectorexporter ]
      metrics/monitoring:
        receivers: [prometheus/monitoring]
        processors: []
        exporters: [otlphttp]

    telemetry:
      logs:
        level: "info"
      metrics:
        level: detailed