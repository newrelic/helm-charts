{{- if .Values.helm.create -}}
{{- $installJobName := include "newrelic.common.naming.truncateToDNSWithSuffix" ( dict "name" (include "newrelic.common.naming.fullname" .) "suffix" "install-job" ) -}}
{{- $configMapName := include "newrelic.common.naming.truncateToDNSWithSuffix" ( dict "name" (include "newrelic.common.naming.fullname" .) "suffix" "job-manifests" ) -}}
{{- /*
To understand why this job installs manifests instead of using Helm, read the comment at `job-manifests.yaml`.
*/ -}}
apiVersion: batch/v1
kind: Job
metadata:
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
    helm.sh/hook-weight: "10010"
  name: {{ $installJobName }}
  namespace: {{ .Release.Namespace }}
spec:
  ttlSecondsAfterFinished: 120
  backoffLimit: 1
  template:
    spec:
      restartPolicy: Never
      serviceAccountName: helm-controller  # Hardcoded on flux2 chart.
      containers:
        - name: apply-crs
          image: bitnami/kubectl
          command: [ "/bin/bash","-c" ]
          args:
          - |
            set -o pipefail

            ERROR_OUTPUT=$(kubectl apply -f /manifests/ 2>&1)
            EXIT_CODE=$?

            # Check if kubectl command was successful or if it contains success keywords
            if [ $EXIT_CODE -ne 0 ] || ! echo "$ERROR_OUTPUT"; then
              echo "Applying CRs failed. Please check the values YAML syntax and Kubernetes resource definitions."
              echo "Error details: $ERROR_OUTPUT"
              exit $EXIT_CODE
            fi

            echo "Manifests applied successfully."
          volumeMounts:
            - name: manifests-configmap
              mountPath: /manifests
      volumes:
        - name: manifests-configmap
          configMap:
            name: {{ $configMapName }}
{{- end }}
